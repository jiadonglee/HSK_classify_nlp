{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSK语料级别自动分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据集预处理、特征工程和模型训练所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textblob, string, jieba, re, os, sys\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/76/5s479vb92j38cy72xv2th3vm0000gn/T/jieba.cache\n",
      "Loading model cost 0.707 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "data = open('train.txt').read()\n",
    "labels, qs_class, texts = [], [], []\n",
    "content_seg = []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])# label\n",
    "    qs_class.append(content[1])# 题型\n",
    "    texts.append(re.sub('\\W', '', content[2]))# 未分词的文本\n",
    "    content_temp = jieba.cut(texts[i]) # 分词处理\n",
    "    content_seg.append(\" \".join(content_temp))# 分词存入content_seg\n",
    "\n",
    "trainDF = pd.DataFrame()\n",
    "labels.pop(0) # 删除数据第一行的“级别”,“text”\n",
    "content_seg.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载HSK词汇表，作为另一部分的训练数据\n",
    "HSK_df = pd.read_csv('HSK.csv')\n",
    "\n",
    "train_X_add = HSK_df['text']\n",
    "train_y_add = HSK_df['label'].tolist()\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "train_y_add = encoder.fit_transform(train_y_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载中文停用词表\n",
    "word = open('stop_word_zh.txt').read()\n",
    "st_word = []\n",
    "for i, line in enumerate(word.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    st_word.append(content[0])\n",
    "      \n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(st_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用TfidfVectorizer初始化向量空间模型\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, \\\n",
    "                             decode_error = 'ignore', stop_words = my_stop_words)\n",
    "transformer = TfidfTransformer()# 统计每个词语的TF-IDF权值\n",
    "\n",
    "# 文本转化为词频矩阵\n",
    "content_seg.extend(train_X_add)\n",
    "labels.extend(train_y_add)\n",
    "content_tdm = vectorizer.fit_transform(content_seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集分为训练集和验证集\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(content_tdm, labels, test_size = 0.2)\n",
    "train_y = encoder.fit_transform(train_y) # label编码为目标变量\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 训练分类器\n",
    "clf = MultinomialNB().fit(train_X, train_y)\n",
    "\n",
    "pred_y = clf.predict(test_X)\n",
    "\n",
    "def metrics_result(actual, predict):\n",
    "    print('accuracy:{0:.3f}'.format(metrics.precision_score(actual, predict, average='weighted')))\n",
    "#     print('recall:{0:0.3f}'.format(metrics.recall_score(actual, predict, average='weighted')))\n",
    "\n",
    "metrics_result(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测结果\n",
    "data_ans = open('test.txt').read()\n",
    "qs_ans, texts_out, texts_ans, temp_ans, seg_ans = [], [], [], [], []\n",
    "for i, line in enumerate(data_ans.split(\"\\n\")):\n",
    "    content_ans = line.split()\n",
    "    qs_ans.append(content_ans[0])\n",
    "    texts_out.append(content_ans[1])\n",
    "    texts_ans.append(re.sub('\\W', '', content_ans[1]))\n",
    "    temp_ans = jieba.cut(texts_ans[i]) # 分词处理\n",
    "    seg_ans.append(\" \".join(temp_ans))# 分词存入content_seg\n",
    "seg_ans.pop(0)\n",
    "\n",
    "ans_tdm = vectorizer.transform(seg_ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
